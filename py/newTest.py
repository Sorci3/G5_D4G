"""
Fine-tuning de EleutherAI/pythia-70m-deduped sur XSum pour des r√©sum√©s de 10-15 mots
"""

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
import numpy as np
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Configuration
MODEL_NAME = "EleutherAI/pythia-70m-deduped"
OUTPUT_DIR = "./pythia-70m-xsum-summarizer"
MAX_LENGTH = 256
TARGET_SUMMARY_LENGTH = "10-15 word"



print("üîß Chargement du mod√®le et du tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto"
)


# Configuration LoRA pour un fine-tuning efficace
print("‚öôÔ∏è Configuration de LoRA...")
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["query_key_value"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

#model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()



# Chargement du dataset XSum
print("üìö Chargement du dataset XSum...")
dataset = load_dataset("xsum", split="train",trust_remote_code=True)  # Limit√© pour l'exemple
eval_dataset = load_dataset("xsum", split="validation")

print(f"Nombre d'exemples d'entra√Ænement: {len(dataset)}")
print(f"Nombre d'exemples de validation: {len(eval_dataset)}")

# Fonction de pr√©paration des donn√©es
def preprocess_function(examples):
    """
    Formate les donn√©es au format instruction-following
    """
    inputs = []
    for document, summary in zip(examples["document"], examples["summary"]):
        # Format prompt clair pour le r√©sum√© court
        prompt = f"""### Instruction:
R√©sume le texte suivant en {TARGET_SUMMARY_LENGTH} maximum.

### Texte:
{document[:500]}

### R√©sum√©:
{summary}"""
        inputs.append(prompt)
    
    # Tokenization
    model_inputs = tokenizer(
        inputs,
        max_length=MAX_LENGTH,
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    )
    
    # Les labels sont identiques aux inputs pour le language modeling
    model_inputs["labels"] = model_inputs["input_ids"].clone()
    
    return model_inputs

# Pr√©paration des datasets
print("üîÑ Pr√©paration des donn√©es...")
tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Tokenization du dataset d'entra√Ænement"
)

tokenized_eval_dataset = eval_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=eval_dataset.column_names,
    desc="Tokenization du dataset de validation"
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Arguments d'entra√Ænement
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    weight_decay=0.01,
    warmup_steps=100,
    logging_steps=50,
    eval_steps=500,
    save_steps=1000,
    eval_strategy="steps",
    save_strategy="steps",
    load_best_model_at_end=True,
    fp16=True,
    fp16_full_eval=True,
    report_to="none",
    remove_unused_columns=False,

)

# Cr√©ation du Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_eval_dataset,
    data_collator=data_collator,
)

# Entra√Ænement
print("üöÄ D√©but de l'entra√Ænement...")
trainer.train()

# Sauvegarde du mod√®le
print("üíæ Sauvegarde du mod√®le...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"‚úÖ Mod√®le sauvegard√© dans {OUTPUT_DIR}")

# Fonction de g√©n√©ration pour tester
def generate_summary(text, max_new_tokens=20):
    """
    G√©n√®re un r√©sum√© court √† partir d'un texte
    """
    prompt = f"""### Instruction:
R√©sume le texte suivant en {TARGET_SUMMARY_LENGTH} maximum.

### Texte:
{text[:500]}

### R√©sum√©:
"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extraire uniquement le r√©sum√© g√©n√©r√©
    summary = full_text.split("### R√©sum√©:")[-1].strip()
    return summary

# Test sur un exemple
print("\nüìù Test de g√©n√©ration:")
test_text = eval_dataset[0]["document"]
print(f"Texte original (extrait): {test_text[:200]}...")
print(f"\nR√©sum√© g√©n√©r√©: {generate_summary(test_text)}")
print(f"R√©sum√© r√©f√©rence: {eval_dataset[0]['summary']}")